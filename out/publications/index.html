<!DOCTYPE html><!--3Uub4QNFk3tGLPNGrG9WN--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/67d2d86283d291eb.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b894f21b5a870c85.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-d92e69f3ed28ec44.js" async=""></script><script src="/_next/static/chunks/main-app-dfcf1868335d7dde.js" async=""></script><script src="/_next/static/chunks/247-27b93a0bc11c40ab.js" async=""></script><script src="/_next/static/chunks/619-3ba632d834111881.js" async=""></script><script src="/_next/static/chunks/918-930ec979fb2e89b3.js" async=""></script><script src="/_next/static/chunks/app/layout-c0a73a7deaa786bc.js" async=""></script><script src="/_next/static/chunks/557-6332093812f5a0a4.js" async=""></script><script src="/_next/static/chunks/681-bd7c3298fd366f1d.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-1f471ce5c689b17e.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><title>Publications | Cheng Yang (杨承)</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Cheng Yang"/><meta name="keywords" content="Cheng Yang,PhD,Research,Hangzhou Dianzi University"/><meta name="creator" content="Cheng Yang"/><meta name="publisher" content="Cheng Yang"/><meta property="og:title" content="Cheng Yang (杨承)"/><meta property="og:description" content="Undergraduate student at Hangzhou Dianzi University."/><meta property="og:site_name" content="Cheng Yang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Cheng Yang (杨承)"/><meta name="twitter:description" content="Undergraduate student at Hangzhou Dianzi University."/><link rel="icon" href="/favicon.svg"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div hidden=""><!--$--><!--/$--></div><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">ImYangC7</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-_R_5pdb_" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">AAAI</span><img alt="LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/lungnodule.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2511.21042" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Hui Jin</span>, </span><span><span class=" ">Xinlei Yu</span>, </span><span><span class=" ">Zhipeng Wang</span>, </span><span><span class=" ">Yaoqun Liu</span>, </span><span><span class=" ">Fenglei Fan</span>, </span><span><span class=" ">Dajiang Lei</span>, </span><span><span class=" ">Gangyong Jia</span>, </span><span><span class=" ">Changmiao Wang</span>, </span><span><span class=" ">Ruiquan Ge</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">AAAI Conference on Artificial Intelligence (AAAI)<!-- -->, <!-- -->2026</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">arXiv</span><img alt="AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/autoenv.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2511.19304" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Jiayi Zhang</span>, </span><span><span class=" ">Yiran Peng</span>, </span><span><span class=" ">Fanqi Kong</span>, </span><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Yifan Wu</span>, </span><span><span class=" ">Zhaoyang Yu</span>, </span><span><span class=" ">Jinyu Xiang</span>, </span><span><span class=" ">Jianhao Ruan</span>, </span><span><span class=" ">Jinlin Wang</span>, </span><span><span class=" ">Maojia Song</span>, </span><span><span class=" ">HongZhang Liu</span>, </span><span><span class=" ">Xiangru Tang</span>, </span><span><span class=" ">Bang Liu</span>, </span><span><span class=" ">Chenglin Wu</span>, </span><span><span class=" ">Yuyu Luo</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2511.19304<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">arXiv</span><img alt="Reasoning via Video: The First Evaluation of Video Models&#x27; Reasoning Abilities through Maze-Solving Tasks" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/vrbench.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2511.15065" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">Reasoning via Video: The First Evaluation of Video Models&#x27; Reasoning Abilities through Maze-Solving Tasks</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Haiyuan Wan</span>, </span><span><span class=" ">Yiran Peng</span>, </span><span><span class=" ">Xin Cheng</span>, </span><span><span class=" ">Zhaoyang Yu</span>, </span><span><span class=" ">Jiayi Zhang</span>, </span><span><span class=" ">Junchi Yu</span>, </span><span><span class=" ">Xinlei Yu</span>, </span><span><span class=" ">Xiawu Zheng</span>, </span><span><span class=" ">Dongzhan Zhou</span>, </span><span><span class=" ">Chenglin Wu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2511.15065<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">arXiv</span><img alt="From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/chemmas.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2509.23768" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Jiaxuan Lu</span>, </span><span><span class=" ">Haiyuan Wan</span>, </span><span><span class=" ">Junchi Yu</span>, </span><span><span class=" ">Feiwei Qin</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2509.23768<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">BIBM Oral</span><img alt="RTGMFF: Enhanced fmri-based brain disorder diagnosis via roi-driven text generation and multimodal feature fusion" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/rtgmff.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2509.03214" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">RTGMFF: Enhanced fmri-based brain disorder diagnosis via roi-driven text generation and multimodal feature fusion</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Junhao Jia</span>, </span><span><span class=" ">Yifei Sun</span>, </span><span><span class=" ">Yunyou Liu</span>, </span><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Changmiao Wang</span>, </span><span><span class=" ">Feiwei Qin</span>, </span><span><span class=" ">Yong Peng</span>, </span><span><span class=" ">Wenwen Min</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE International Conference on Bioinformatics and Biomedicine (BIBM)<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">arXiv</span><img alt="Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/brainhcn.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2509.14965" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Junhao Jia</span>, </span><span><span class=" ">Yunyou Liu</span>, </span><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Yifei Sun</span>, </span><span><span class=" ">Feiwei Qin</span>, </span><span><span class=" ">Changmiao Wang</span>, </span><span><span class=" ">Yong Peng</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2509.14965<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">arXiv</span><img alt="Visual Document Understanding and Reasoning: A Multi-Agent Collaboration Framework with Agent-Wise Adaptive Test-Time Scaling" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/mact.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://arxiv.org/abs/2508.03404" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">Visual Document Understanding and Reasoning: A Multi-Agent Collaboration Framework with Agent-Wise Adaptive Test-Time Scaling</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Xinlei Yu</span>, </span><span><span class=" ">Chengming Xu</span>, </span><span><span class=" ">Zhangquan Chen</span>, </span><span><span class=" ">Yudong Zhang</span>, </span><span><span class=" ">Shilin Lu</span>, </span><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Jiangning Zhang</span>, </span><span><span class=" ">Shuicheng Yan</span>, </span><span><span class=" ">Xiaobin Hu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2508.03404<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">Frontiers</span><img alt="PI-MMNet: a cross-modal neural network for predicting neurological deterioration in pontine infarction" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/PI-MMNet.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2025.1637079/full" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">PI-MMNet: a cross-modal neural network for predicting neurological deterioration in pontine infarction</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Hui Jin</span>, </span><span><span class=" ">Xiaona Xu</span>, </span><span><span class=" ">Yichan Ye</span>, </span><span><span class=" ">Xuhao Shan</span>, </span><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Min Li</span>, </span><span><span class=" ">Enyu Bao</span>, </span><span><span class=" ">Weili Chen</span>, </span><span><span class=" ">Xuerong Huang</span>, </span><span><span class=" ">Jikui Liu</span>, </span><span><span class=" ">others</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Frontiers in Neuroscience<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-52 flex-shrink-0"><div class="relative p-1 rounded-lg bg-white dark:bg-neutral-700 shadow-md hover:shadow-xl transition-shadow duration-300 ring-1 ring-neutral-200 dark:ring-neutral-600 cursor-zoom-in"><span class="absolute top-0 left-0 z-10 px-2 py-1 text-xs font-bold text-white bg-blue-900 rounded-br-md shadow-md" style="font-family:Arial, sans-serif">MICCAI</span><img alt="Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification" loading="lazy" width="300" height="200" decoding="async" data-nimg="1" class="w-full h-auto rounded hover:scale-105 transition-transform duration-300" style="color:transparent" src="/papers/Lesions-aware.jpg"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight"><a href="https://link.springer.com/chapter/10.1007/978-3-032-04927-8_56" target="_blank" rel="noopener noreferrer" class="hover:text-accent transition-colors duration-200">Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification</a></h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Jianxun Yu</span>, </span><span><span class=" ">Ruiquan Ge</span>, </span><span><span class=" ">Zhipeng Wang</span>, </span><span><span class="font-semibold text-accent ">Cheng Yang</span>, </span><span><span class=" ">Chenyu Lin</span>, </span><span><span class=" ">Xianjun Fu</span>, </span><span><span class=" ">Jikui Liu</span>, </span><span><span class=" ">Ahmed Elazab</span>, </span><span><span class=" ">Changmiao Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)<!-- -->, <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div></div></div></div><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-center items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->December 5, 2025</p></div></div></footer></div><script src="/_next/static/chunks/webpack-b894f21b5a870c85.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[7558,[\"247\",\"static/chunks/247-27b93a0bc11c40ab.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"918\",\"static/chunks/918-930ec979fb2e89b3.js\",\"177\",\"static/chunks/app/layout-c0a73a7deaa786bc.js\"],\"ThemeProvider\"]\n3:I[9994,[\"247\",\"static/chunks/247-27b93a0bc11c40ab.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"918\",\"static/chunks/918-930ec979fb2e89b3.js\",\"177\",\"static/chunks/app/layout-c0a73a7deaa786bc.js\"],\"default\"]\n4:I[9766,[],\"\"]\n5:I[8924,[],\"\"]\nc:I[7150,[],\"\"]\n:HL[\"/_next/static/css/67d2d86283d291eb.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"3Uub4QNFk3tGLPNGrG9WN\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/67d2d86283d291eb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\",\"suppressHydrationWarning\":true}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"ImYangC7\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],\"$L6\",\"$L7\"]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],\"$L8\"]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],\"$L9\",{\"children\":[\"__PAGE__\",\"$La\",{},null,false]},null,false]},null,false],\"$Lb\",false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[7923,[\"247\",\"static/chunks/247-27b93a0bc11c40ab.js\",\"619\",\"static/chunks/619-3ba632d834111881.js\",\"918\",\"static/chunks/918-930ec979fb2e89b3.js\",\"177\",\"static/chunks/app/layout-c0a73a7deaa786bc.js\"],\"default\"]\nf:I[4431,[],\"OutletBoundary\"]\n11:I[5278,[],\"AsyncMetadataOutlet\"]\n13:I[4431,[],\"ViewportBoundary\"]\n15:I[4431,[],\"MetadataBoundary\"]\n16:\"$Sreact.suspense\"\n6:[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}]\n7:[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]\n8:[\"$\",\"$Ld\",null,{\"lastUpdated\":\"December 5, 2025\"}]\n9:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\na:[\"$\",\"$1\",\"c\",{\"children\":[\"$Le\",null,[\"$\",\"$Lf\",null,{\"children\":[\"$L10\",[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]]}]]}]\nb:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L13\",null,{\"children\":\"$L14\"}],null],[\"$\",\"$L15\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$16\",null,{\"fallback\":null,\"children\":\"$L17\"}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"18:I[9958,[\"247\",\"static/chunks/247-27b93a0bc11c40ab.js\",\"557\",\"static/chunks/557-6332093812f5a0a4.js\",\"681\",\"static/chunks/681-bd7c3298fd366f1d.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-1f471ce5c689b17e.js\"],\"default\"]\n"])</script><script>self.__next_f.push([1,"e:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L18\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"yang2025lungnoduleagent\",\"title\":\"LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules\",\"authors\":[{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hui Jin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinlei Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhipeng Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yaoqun Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fenglei Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dajiang Lei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Gangyong Jia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changmiao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruiquan Ge\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"month\":\"1\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:0:tags\",\"researchArea\":\"reliability-engineering\",\"journal\":\"AAAI Conference on Artificial Intelligence (AAAI)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2511.21042\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"lungnodule.jpg\",\"venue\":\"AAAI\",\"bibtex\":\"@article{yang2025lungnoduleagent,\\n  title = {LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules},\\n  author = {Yang, Cheng and Jin, Hui and Yu, Xinlei and Wang, Zhipeng and Liu, Yaoqun and Fan, Fenglei and Lei, Dajiang and Jia, Gangyong and Wang, Changmiao and Ge, Ruiquan},\\n  journal = {AAAI Conference on Artificial Intelligence (AAAI)},\\n  year = {2026},\\n  url = {https://arxiv.org/abs/2511.21042}\\n}\"},{\"id\":\"zhang2025autoenv\",\"title\":\"AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning\",\"authors\":[{\"name\":\"Jiayi Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiran Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fanqi Kong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yifan Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhaoyang Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jinyu Xiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianhao Ruan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jinlin Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Maojia Song\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"HongZhang Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiangru Tang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bang Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chenglin Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuyu Luo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2511.19304\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2511.19304\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"autoenv.jpg\",\"venue\":\"arXiv\",\"bibtex\":\"@article{zhang2025autoenv,\\n  title = {AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning},\\n  author = {Jiayi Zhang and Yiran Peng and Fanqi Kong and Cheng Yang and Yifan Wu and Zhaoyang Yu and Jinyu Xiang and Jianhao Ruan and Jinlin Wang and Maojia Song and HongZhang Liu and Xiangru Tang and Bang Liu and Chenglin Wu and Yuyu Luo},\\n  journal = {arXiv preprint arXiv:2511.19304},\\n  year = {2025},\\n  url = {https://arxiv.org/abs/2511.19304}\\n}\"},{\"id\":\"yang2025vrbench\",\"title\":\"Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks\",\"authors\":[{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haiyuan Wan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiran Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Cheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhaoyang Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiayi Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junchi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinlei Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiawu Zheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dongzhan Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chenglin Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2511.15065\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2511.15065\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"vrbench.jpg\",\"venue\":\"arXiv\",\"bibtex\":\"@article{yang2025vrbench,\\n  title = {Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks},\\n  author = {Cheng Yang and Haiyuan Wan and Yiran Peng and Xin Cheng and Zhaoyang Yu and Jiayi Zhang and Junchi Yu and Xinlei Yu and Xiawu Zheng and Dongzhan Zhou and Chenglin Wu},\\n  journal = {arXiv preprint arXiv:2511.15065},\\n  year = {2025},\\n  url = {https://arxiv.org/abs/2511.15065}\\n}\"},{\"id\":\"yang2025multi\",\"title\":\"From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning\",\"authors\":[{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaxuan Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haiyuan Wan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junchi Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feiwei Qin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2509.23768\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2509.23768\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"chemmas.jpg\",\"venue\":\"arXiv\",\"bibtex\":\"@article{yang2025multi,\\n  title = {From What to Why: A Multi-Agent System for Evidence-based Chemical Reaction Condition Reasoning},\\n  author = {Yang, Cheng and Lu, Jiaxuan and Wan, Haiyuan and Yu, Junchi and Qin, Feiwei},\\n  journal = {arXiv preprint arXiv:2509.23768},\\n  year = {2025},\\n  url = {https://arxiv.org/abs/2509.23768}\\n}\"},{\"id\":\"jia2025rtgmff\",\"title\":\"RTGMFF: Enhanced fmri-based brain disorder diagnosis via roi-driven text generation and multimodal feature fusion\",\"authors\":[{\"name\":\"Junhao Jia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yifei Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yunyou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changmiao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feiwei Qin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yong Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenwen Min\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:4:tags\",\"researchArea\":\"reliability-engineering\",\"journal\":\"IEEE International Conference on Bioinformatics and Biomedicine (BIBM)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2509.03214\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"rtgmff.jpg\",\"venue\":\"BIBM Oral\",\"bibtex\":\"@article{jia2025rtgmff,\\n  title = {RTGMFF: Enhanced fmri-based brain disorder diagnosis via roi-driven text generation and multimodal feature fusion},\\n  author = {Jia, Junhao and Sun, Yifei and Liu, Yunyou and Yang, Cheng and Wang, Changmiao and Qin, Feiwei and Peng, Yong and Min, Wenwen},\\n  journal = {IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},\\n  year = {2025},\\n  url = {https://arxiv.org/abs/2509.03214}\\n}\"},{\"id\":\"jia2025brain\",\"title\":\"Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis\",\"authors\":[{\"name\":\"Junhao Jia\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yunyou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yifei Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feiwei Qin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changmiao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yong Peng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:5:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2509.14965\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2509.14965\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"brainhcn.jpg\",\"venue\":\"arXiv\",\"bibtex\":\"@article{jia2025brain,\\n  title = {Brain-HGCN: A Hyperbolic Graph Convolutional Network for Brain Functional Network Analysis},\\n  author = {Jia, Junhao and Liu, Yunyou and Yang, Cheng and Sun, Yifei and Qin, Feiwei and Wang, Changmiao and Peng, Yong},\\n  journal = {arXiv preprint arXiv:2509.14965},\\n  year = {2025},\\n  url = {https://arxiv.org/abs/2509.14965}\\n}\"},{\"id\":\"yu2025visualdocumentunderstandingreasoning\",\"title\":\"Visual Document Understanding and Reasoning: A Multi-Agent Collaboration Framework with Agent-Wise Adaptive Test-Time Scaling\",\"authors\":[{\"name\":\"Xinlei Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengming Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhangquan Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yudong Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shilin Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiangning Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuicheng Yan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaobin Hu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"8\",\"type\":\"preprint\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:6:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"arXiv preprint arXiv:2508.03404\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2508.03404\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"mact.jpg\",\"venue\":\"arXiv\",\"bibtex\":\"@misc{yu2025visualdocumentunderstandingreasoning,\\n  title = {Visual Document Understanding and Reasoning: A Multi-Agent Collaboration Framework with Agent-Wise Adaptive Test-Time Scaling},\\n  author = {Xinlei Yu and Chengming Xu and Zhangquan Chen and Yudong Zhang and Shilin Lu and Cheng Yang and Jiangning Zhang and Shuicheng Yan and Xiaobin Hu},\\n  journal = {arXiv preprint arXiv:2508.03404},\\n  year = {2025},\\n  url = {https://arxiv.org/abs/2508.03404}\\n}\"},{\"id\":\"jin2025pi\",\"title\":\"PI-MMNet: a cross-modal neural network for predicting neurological deterioration in pontine infarction\",\"authors\":[{\"name\":\"Hui Jin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaona Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yichan Ye\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuhao Shan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Min Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Enyu Bao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Weili Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuerong Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jikui Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"others\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"8\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:7:tags\",\"researchArea\":\"neural-networks\",\"journal\":\"Frontiers in Neuroscience\",\"conference\":\"\",\"volume\":\"19\",\"pages\":\"1637079\",\"url\":\"https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2025.1637079/full\",\"abstract\":\"\",\"description\":\"\",\"selected\":false,\"preview\":\"PI-MMNet.jpg\",\"venue\":\"Frontiers\",\"bibtex\":\"@article{jin2025pi,\\n  title = {PI-MMNet: a cross-modal neural network for predicting neurological deterioration in pontine infarction},\\n  author = {Jin, Hui and Xu, Xiaona and Ye, Yichan and Shan, Xuhao and Yang, Cheng and Li, Min and Bao, Enyu and Chen, Weili and Huang, Xuerong and Liu, Jikui and others},\\n  journal = {Frontiers in Neuroscience},\\n  volume = {19},\\n  pages = {1637079},\\n  year = {2025},\\n  publisher = {Frontiers},\\n  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2025.1637079/full}\\n}\"},{\"id\":\"yu2025small\",\"title\":\"Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification\",\"authors\":[{\"name\":\"Jianxun Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruiquan Ge\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhipeng Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Cheng Yang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chenyu Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xianjun Fu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jikui Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ahmed Elazab\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changmiao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"4\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$e:props:children:0:props:publications:8:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)\",\"pages\":\"589--598\",\"url\":\"https://link.springer.com/chapter/10.1007/978-3-032-04927-8_56\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"Lesions-aware.jpg\",\"venue\":\"MICCAI\",\"bibtex\":\"@article{yu2025small,\\n  title = {Small Lesions-aware Bidirectional Multimodal Multiscale Fusion Network for Lung Disease Classification},\\n  author = {Yu, Jianxun and Ge, Ruiquan and Wang, Zhipeng and Yang, Cheng and Lin, Chenyu and Fu, Xianjun and Liu, Jikui and Elazab, Ahmed and Wang, Changmiao},\\n  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},\\n  pages = {589--598},\\n  year = {2025},\\n  organization = {Springer},\\n  url = {https://link.springer.com/chapter/10.1007/978-3-032-04927-8_56}\\n}\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"14:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n10:null\n"])</script><script>self.__next_f.push([1,"19:I[622,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"12:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Cheng Yang (杨承)\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Cheng Yang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Cheng Yang,PhD,Research,Hangzhou Dianzi University\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Cheng Yang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Cheng Yang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Cheng Yang (杨承)\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Undergraduate student at Hangzhou Dianzi University.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Cheng Yang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Cheng Yang (杨承)\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Undergraduate student at Hangzhou Dianzi University.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}],[\"$\",\"$L19\",\"15\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"17:\"$12:metadata\"\n"])</script></body></html>